# -*- coding: utf-8 -*-
"""lesson14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dBgoOyUskHNQe4wbGrR4Lt7EBDzetgOg
"""

!pip install tensorflow

import os # операционная система (работа с папкой)
import requests # запросы по url
import re # регулярные выражения
from bs4 import BeautifulSoup # разобрать html
import io # работа с файлами (загрузка / выгрузка)
import shutil # переместить из папки в папку

# from google.colab import files # скачать из colab

import pandas as pd

# Классификация нового изображения
from PIL import Image
from torchvision import transforms
import torch

import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.optimizers import SGD

from sklearn.model_selection import train_test_split

# Загрузка данных
GENERAL_URL = 'https://yandex.ru/images/search?text={query_text}&p={page}'
DIR_NAME_class01 = 'пиджак'
DIR_NAME_class02 = 'брюки'
ENCODING = 'utf-8'
MOZILLA_SEARCH_HEADER = {'User-Agent':'Mozilla/5.0'} # Браузер Мозилла
HTML_MODE = 'html.parser'
HREF_MODE = True # включить запись ссылок

# Функция парсера URL
def parser_url(url):
  pattern = r'img_url=([^&]+)&text='
  match_u = re.search(pattern, url)
  if match_u:
    img_url_encoded = match_u.group(1)
    img_url_decoded = img_url_encoded.replace('%2F', '/').replace('%3A', ':')
    print(img_url_decoded)
    return img_url_decoded
  else:
    print('Ссылка после img_url не найдена в URL')

# Функция загрузки 1 изображения
def download_image(url, save_path):
  try:
    response = requests.get(url, headers=MOZILLA_SEARCH_HEADER, stream=True)
    if(response.status_code == 200):
      with open(save_path, 'wb') as file:
        for chunk in response.iter_content(1024):
          file.write(chunk)
      return True
    else:
      print(f'Не удалось загрузить изображение: {url}')
      return False
  except Exception as e:
    print(f'Ошибка при загрузке изображения: {url}')
    return False

def get_links_on_keyword(keyword, page_num=1, encoding=ENCODING):
  '''
  Взять по ключевому слову набор ссылок с сайта ХабраХабр
  @param keyword - ключевое слово
  @param page_num - сколько страниц смотрим (как правило, на 1 странице 20 статей)
  '''
  links_to_find = []
  names_to_find = []
  __PARSE_TAG = 'a'
  __PARSE_CLASS = 'serp-item__link'

  total_len = page_num
  keyword_lwr = keyword.lower()
  downloaded_count = 0

  for page in range(page_num + 1):
    page_articles = GENERAL_URL.format(page=page_num, query_text=keyword)
    response = requests.get(page_articles, headers=MOZILLA_SEARCH_HEADER) # Браузер Мозилла
    response.encoding = encoding
    page_data_soup = BeautifulSoup(response.text, HTML_MODE)
    if(response.status_code == 200):
      for link in page_data_soup.find_all(__PARSE_TAG, HREF_MODE, class_=__PARSE_CLASS):
        print(link['href'])
        # получаем полный URL изображения
        img_url = parser_url(link['href'])
        print(img_url)
        links_to_find.append[img_url]
  return names_to_find, links_to_find

# Функция проверки директории
def check_repo_dataset(class_name, current_dir="./"):
  class_folder = os.path.join(current_dir + '/datasets', class_name)
  if not os.path.exists(class_folder):
    os.makedirs(class_folder)
    return class_folder
  else:
    return class_folder

# Функция колличество изображений в выдаче
def calc_pages(num_images):
  return num_images // 5 + (num_images % 5 > 0) if num_images > 5 else 1

# Функция загрузки изображений
def download_images(query, num_images, mini_images = False):
  pages = calc_pages(num_images)
  class_folder = check_repo_dataset(query, "./")
  downloaded_count = 0
  base_url = 'https:'
  # а вот это чтобы без движков было, грузим странички
  for page in range(0, pages):
    search_url = GENERAL_URL.format(query_text=query, page=page)
    response = requests.get(search_url, headers=MOZILLA_SEARCH_HEADER)
    soup = BeautifulSoup(response.text, 'html.parser')
    if (mini_images == False):
      for a in soup.find_all('a', class_='serp-item__link'):
        img_url = a['href']
        # получаем полный URL изображения
        img_url = parser_url(img_url)
        image_filename = f'{downloaded_count:04d}.jpg'
        image_path = os.path.join(class_folder, image_filename)
        if(download_image(img_url, image_path)):
          downloaded_count += 1
          print(f"Загружено изображений для {query}: {downloaded_count}/{num_images}")
        if(downloaded_count >= num_images):
          break
    else:
      for a in soup.find_all('img', class_='serp-item__thumb'):
        img_url = a['src']
        # из за получение //avatar, надо бы добавить https:// чтобы ссылка стала полной
        if(not img_url.startswith('http')):
          img_url = base_url + img_url
          image_filename = f'{downloaded_count:04d}.jpg'
          image_path = os.path.join(class_folder, image_filename)
          if(download_image(img_url, image_path)):
            downloaded_count += 1
            print(f'Загружено изображений для {query}: {downloaded_count}/{num_images}')
          if(downloaded_count >= num_images):
            break

# Загрузка 1 набора изображений
download_images(DIR_NAME_class01, num_images=80, mini_images=True)

# Загрузка 2 набора изображений
download_images(DIR_NAME_class02, num_images=80, mini_images=True)

!zip -r '/content/{DIR_NAME_class01}.zip' '/content/datasets/{DIR_NAME_class01}'

!zip -r '/content/{DIR_NAME_class02}.zip' '/content/datasets/{DIR_NAME_class02}'

# Распаковка архива датасета
# !unzip -q '/content/{DIR_NAME_class01}.zip'
# !unzip -q '/content/{DIR_NAME_class02}.zip'

# Получить файлы
folder = '/content/datasets/'
lst_files_01 = os.listdir(f'{folder}/{DIR_NAME_class01}')[:50] # 1. Считываем из папки; 2. Обрезаем датасет, 1
lst_files_02 = os.listdir(f'{folder}/{DIR_NAME_class02}')[:50] # 1. Считываем из папки; 2. Обрезаем датасет, 0

print(len(lst_files_01))
print(len(lst_files_02))

# Изменить имена файлов
image_file_path_01 = [f"{DIR_NAME_class01}_{file_name}" for file_name in lst_files_01]
image_file_path_02 = [f"{DIR_NAME_class02}_{file_name}" for file_name in lst_files_02]

# Собрать в датафрейм
df_class_01 = pd.DataFrame({'image_file_path': image_file_path_01, 'image_real_path': lst_files_01, 'target': 1})
df_class_02 = pd.DataFrame({'image_file_path': image_file_path_02, 'image_real_path': lst_files_02, 'target': 0})

df = pd.concat([df_class_01, df_class_02], ignore_index=True) # сложили датафрейм
df

# Для первого класса
X_train, X_test, y_train, y_test = train_test_split(df[['image_file_path', 'image_real_path']], df['target'], test_size=0.2, random_state=1234, stratify=df['target'])
X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1234, stratify=y_test)

X_train

# Длина тренировочной выборки для первого класса
print('Длина тренировочной выборки: ' + str(len(y_train[y_train == 0])))

# Длина тестовой выборки для первого класса
print('Длина тестовой выборки: ' + str(len(y_test[y_test == 0])))

# Длина валидационной выборки для первого класса
print('Длина валидационной выборки: ' + str(len(y_val[y_val == 0])))

# Добавляем столбец с классом для тренировочного датасета
X_train_full = X_train.copy()
X_train_full['target'] = y_train

X_train_full

# Добавляем столбец с классом для тестового датасета
X_test_full = X_test.copy()
X_test_full['target'] = y_test

X_test_full

# Добавляем столбец с классом для валидационного датасета
X_val_full = X_val.copy()
X_val_full['target'] = y_val

X_val_full

# Создаем необходимые директории
!mkdir "train_data"
!mkdir "test_data"
!mkdir "validation_data"

!mkdir '/content/train_data/{DIR_NAME_class01}/'
!mkdir '/content/train_data/{DIR_NAME_class02}/'

!mkdir '/content/test_data/{DIR_NAME_class01}/'
!mkdir '/content/test_data/{DIR_NAME_class02}/'

!mkdir '/content/validation_data/{DIR_NAME_class01}/'
!mkdir '/content/validation_data/{DIR_NAME_class02}/'

# Функция перемещения из папки в папку
def move_img_to_folder(path, path_to_move, url_file):
  shutil.move(f"{path}/{url_file}", path_to_move)

# Перемещаем данные
for index, row in X_train_full.iterrows():
  folder_from = f'/content/datasets/{DIR_NAME_class01}/' if row['target'] == 1 else f'/content/datasets/{DIR_NAME_class02}'
  folder_to_move = f'/content/train_data/{DIR_NAME_class01}/' if row['target'] == 1 else f'/content/train_data/{DIR_NAME_class02}'
  move_img_to_folder(folder_from, folder_to_move, row['image_real_path'])

for index, row in X_test_full.iterrows():
  folder_from = f'/content/datasets/{DIR_NAME_class01}/' if row['target'] == 1 else f'/content/datasets/{DIR_NAME_class02}'
  folder_to_move = f'/content/test_data/{DIR_NAME_class01}/' if row['target'] == 1 else f'/content/test_data/{DIR_NAME_class02}'
  move_img_to_folder(folder_from, folder_to_move, row['image_real_path'])

for index, row in X_val_full.iterrows():
  folder_from = f'/content/datasets/{DIR_NAME_class01}/' if row['target'] == 1 else f'/content/datasets/{DIR_NAME_class02}'
  folder_to_move = f'/content/validation_data/{DIR_NAME_class01}/' if row['target'] == 1 else f'/content/validation_data/{DIR_NAME_class02}'
  move_img_to_folder(folder_from, folder_to_move, row['image_real_path'])

!zip -r '/content/train_data.zip' '/content/train_data/'
!zip -r '/content/test_data.zip' '/content/test_data/'
!zip -r '/content/validation_data.zip' '/content/validation_data/'

def classify_image(image_path, device, model, class_names):
  image = Image.open(image_path)
  preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
  ])
  image = preprocess(image).unsqueeze(0)  # Добавление размерности батча (batch dimension)
  image = image.to(device)

  model.eval()
  with torch.no_grad():
    outputs = model(image)
    _, predicted = torch.max(outputs, 1)

  return class_names[predicted[0]]

# Генерация данных
# Задаём параметры для предобработки изображений
image_size = (224, 224) # сложности с обработкой, (28,28) - сжатие изображения, при сжатии остаются основные моменты, компьютер осталяет самое важное
batch_size = 40 # 16, 32 для текста, для изображений больше 32
data = '/content'

# Создаем генератоы данных для тренировочного, валидационного и тестового наборов
train_datagen = ImageDataGenerator(
  rescale=1.0 / 255.0,
  rotation_range=20,
  width_shift_range=0.2,
  height_shift_range=0.2,
  shear_range=0.2,
  zoom_range=0.2,
  horizontal_flip=True,
  fill_mode='nearest'
)

train_generator = train_datagen.flow_from_directory(
  os.path.join(data, 'train_data'),
  target_size=image_size,
  batch_size=batch_size,
  class_mode='binary'
) # 224*224

test_datagen = ImageDataGenerator(
  rescale=1.0 / 255.0
)

test_generator = test_datagen.flow_from_directory(
  os.path.join(data, 'test_data'),
  target_size=image_size,
  batch_size=1,
  class_mode=None,
  shuffle=False
) # 224*224

validation_datagen = ImageDataGenerator(
  rescale=1.0 / 255.0
)

validation_generator = validation_datagen.flow_from_directory(
  os.path.join(data, 'validation_data'),
  target_size=image_size,
  batch_size=batch_size,
  class_mode='binary'
) # 224*224

# Создание модели
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# Компиляция модели
model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(learning_rate=0.01, momentum=0.99),
    metrics=['accuracy']
)

# Обучаем модель
num_epochs = 30 # Количество эпох
history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

# Сохранение модели
model.save('classification_model.h5')

# тестирование на демонстрационных данных

# создаем функцию classify_image
def classify_images_in_directory(directory_path, output_file):

  # открываем файл для записи результатов
  with open(output_file, 'w') as result_file:

    # получаем список поддиректорий (car и not_car)
    subdirectories = os.listdir(directory_path)

    for subdirectory in subdirectories:
      subdirectory_path = os.path.join(directory_path, subdirectory)

      # проверяем, является ли поддиректория директорией
      if os.path.isdir(subdirectory_path):

        # получаем список файлов в поддиректории
        image_files = os.listdir(subdirectory_path)
        print(image_files)

        for image_file in image_files:

          # полный путь к изображению
          image_path = os.path.join(subdirectory_path, image_file)

          # загружаем и классифицируем изображение
          img = image.load_img(image_path, target_size=(224, 224))
          x = image.img_to_array(img)
          x = x / 255.0
          x = x.reshape((1, 224, 224, 3))
          pred = model.predict(x)

          # определяем метку класса
          if pred[0][0] > 0.5:
              class_label = DIR_NAME_class01
          else:
              class_label = 'НЕ ' + str(DIR_NAME_class01)

          # выводим результат в консоль
          print(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
          print(f'Training Accuracy: {history.history["accuracy"][-1]}')
          print(f'Validation Accuracy: {history.history["val_accuracy"][-1]}\n')

          # записываем результаты в файл
          result_file.write(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
          result_file.write(f'Training Accuracy: {history.history["accuracy"][-1]}\n')
          result_file.write(f'Validation Accuracy: {history.history["val_accuracy"][-1]}\n')


# путь к директории с демонстрационными данными
demo_data_directory = f'/content/test_data/'

# путь к файлу, в который будут записаны результаты классификации
output_file = '/content/test_results.txt'

# вызываем функцию для классификации демонстрационных данных
classify_images_in_directory(demo_data_directory, output_file)

# Полученные результаты тестирования модели на демонстрационных данных показывают, что точность модели хорошая.

# Проведем дополнительные эксперементы

# Эксперимент 1_1 Уменьшение количества эпох с 20 до 10

# создание модели
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# компиляция модели
model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(learning_rate=0.01, momentum=0.99),
    metrics=['accuracy']
)

# уменьшаем количество эпох с 20 до 10
num_epochs = 10

# обучаем модель
history_1_1 = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

# сохраняем модель
model.save('experiment_1_1_model.h5')

# создаем функцию classify_image
def classify_images_in_directory(directory_path, output_file):

    # открываем файл для записи результатов
    with open(output_file, 'w') as result_file:

        # получаем список поддиректорий (car и not_car)
        subdirectories = os.listdir(directory_path)

        for subdirectory in subdirectories:
            subdirectory_path = os.path.join(directory_path, subdirectory)

            # проверяем, является ли поддиректория директорией
            if os.path.isdir(subdirectory_path):

                # получаем список файлов в поддиректории
                image_files = os.listdir(subdirectory_path)

                for image_file in image_files:

                    # полный путь к изображению
                    image_path = os.path.join(subdirectory_path, image_file)

                    # загружаем и классифицируем изображение
                    img = image.load_img(image_path, target_size=(224, 224))
                    x = image.img_to_array(img)
                    x = x / 255.0
                    x = x.reshape((1, 224, 224, 3))
                    pred = model.predict(x)

                    # определяем метку класса
                    if pred[0][0] > 0.5:
                        class_label = DIR_NAME_class01
                    else:
                        class_label = 'НЕ ' + str(DIR_NAME_class01)

                    # выводим результат в консоль
                    print(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    print(f'Training Accuracy: {history_1_1.history["accuracy"][-1]}')
                    print(f'Validation Accuracy: {history_1_1.history["val_accuracy"][-1]}\n')

                    # записываем результаты в файл
                    result_file.write(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    result_file.write(f'Training Accuracy: {history_1_1.history["accuracy"][-1]}\n')
                    result_file.write(f'Validation Accuracy: {history_1_1.history["val_accuracy"][-1]}\n')

# путь к директории с демонстрационными данными
demo_data_directory = f'/content/test_data/'

# путь к файлу, в который будут записаны результаты классификации
output_file_experiment_1_1 = '/content/test_results_experiment_1_1.txt'

# вызываем функцию для классификации
classify_images_in_directory(demo_data_directory, output_file_experiment_1_1)

# Эксперимент 1_2 Уменьшение количества эпох с 20 до 4

# создание модели
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# компиляция модели
model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(learning_rate=0.01, momentum=0.99),
    metrics=['accuracy']
)

# уменьшаем количество эпох с 20 до 4
num_epochs = 4

# обучаем модель
history_1_2 = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

# сохраняем модель
model.save('experiment_1_2_model.h5')

# создаем функцию classify_image
def classify_images_in_directory(directory_path, output_file):

    # открываем файл для записи результатов
    with open(output_file, 'w') as result_file:

        # получаем список поддиректорий (car и not_car)
        subdirectories = os.listdir(directory_path)

        for subdirectory in subdirectories:
            subdirectory_path = os.path.join(directory_path, subdirectory)

            # проверяем, является ли поддиректория директорией
            if os.path.isdir(subdirectory_path):

                # получаем список файлов в поддиректории
                image_files = os.listdir(subdirectory_path)

                for image_file in image_files:

                    # полный путь к изображению
                    image_path = os.path.join(subdirectory_path, image_file)

                    # загружаем и классифицируем изображение
                    img = image.load_img(image_path, target_size=(224, 224))
                    x = image.img_to_array(img)
                    x = x / 255.0
                    x = x.reshape((1, 224, 224, 3))
                    pred = model.predict(x)

                    # определяем метку класса
                    if pred[0][0] > 0.5:
                        class_label = DIR_NAME_class01
                    else:
                        class_label = 'НЕ ' + str(DIR_NAME_class01)

                    # выводим результат в консоль
                    print(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    print(f'Training Accuracy: {history_1_2.history["accuracy"][-1]}')
                    print(f'Validation Accuracy: {history_1_2.history["val_accuracy"][-1]}\n')

                    # записываем результаты в файл
                    result_file.write(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    result_file.write(f'Training Accuracy: {history_1_2.history["accuracy"][-1]}\n')
                    result_file.write(f'Validation Accuracy: {history_1_2.history["val_accuracy"][-1]}\n')

# путь к директории с демонстрационными данными
demo_data_directory = f'/content/test_data/'

# путь к файлу, в который будут записаны результаты классификации
output_file_experiment_1_2 = '/content/test_results_experiment_1_2.txt'

# вызываем функцию для классификации
classify_images_in_directory(demo_data_directory, output_file_experiment_1_2)

# Уменьшение количества эпох приводит к ухудшению и !нестабильности! итоговых показателей модели

# Эксперимент 2_1 Изменение размера пакета = 16 (изначально Batch Size = 40)

batch_size = 16

# создание модели
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# компиляция модели
model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(learning_rate=0.01, momentum=0.99),
    metrics=['accuracy']
)

# обучаем модель
history_2_1 = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

# сохраняем модель
model.save('experiment_2_1_model.h5')

# создаем функцию classify_image
def classify_images_in_directory(directory_path, output_file):

    # открываем файл для записи результатов
    with open(output_file, 'w') as result_file:

        # получаем список поддиректорий (car и not_car)
        subdirectories = os.listdir(directory_path)

        for subdirectory in subdirectories:
            subdirectory_path = os.path.join(directory_path, subdirectory)

            # проверяем, является ли поддиректория директорией
            if os.path.isdir(subdirectory_path):

                # получаем список файлов в поддиректории
                image_files = os.listdir(subdirectory_path)

                for image_file in image_files:

                    # полный путь к изображению
                    image_path = os.path.join(subdirectory_path, image_file)

                    # загружаем и классифицируем изображение
                    img = image.load_img(image_path, target_size=(224, 224))
                    x = image.img_to_array(img)
                    x = x / 255.0
                    x = x.reshape((1, 224, 224, 3))
                    pred = model.predict(x)

                    # определяем метку класса
                    if pred[0][0] > 0.5:
                        class_label = DIR_NAME_class01
                    else:
                        class_label = 'НЕ ' + str(DIR_NAME_class01)

                    # выводим результат в консоль
                    print(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    print(f'Training Accuracy: {history_2_1.history["accuracy"][-1]}')
                    print(f'Validation Accuracy: {history_2_1.history["val_accuracy"][-1]}\n')

                    # записываем результаты в файл
                    result_file.write(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    result_file.write(f'Training Accuracy: {history_2_1.history["accuracy"][-1]}\n')
                    result_file.write(f'Validation Accuracy: {history_2_1.history["val_accuracy"][-1]}\n')

# путь к директории с демонстрационными данными
demo_data_directory = f'/content/test_data/'

# путь к файлу, в который будут записаны результаты классификации
output_file_experiment_2_1 = '/content/test_results_experiment_2_1.txt'

# вызываем функцию для классификации
classify_images_in_directory(demo_data_directory, output_file_experiment_2_1)

# Эксперимент 2_2 Изменение размера пакета = 32 (изначально Batch Size = 40)

batch_size = 32

# создание модели
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# компиляция модели
model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(learning_rate=0.01, momentum=0.99),
    metrics=['accuracy']
)

# обучение модели
history_2_2 = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

# сохраняем модель
model.save('experiment_2_2_model.h5')

# создаем функцию classify_image
def classify_images_in_directory(directory_path, output_file):

    # открываем файл для записи результатов
    with open(output_file, 'w') as result_file:

        # получаем список поддиректорий (car и not_car)
        subdirectories = os.listdir(directory_path)

        for subdirectory in subdirectories:
            subdirectory_path = os.path.join(directory_path, subdirectory)

            # проверяем, является ли поддиректория директорией
            if os.path.isdir(subdirectory_path):

                # получаем список файлов в поддиректории
                image_files = os.listdir(subdirectory_path)

                for image_file in image_files:

                    # полный путь к изображению
                    image_path = os.path.join(subdirectory_path, image_file)

                    # загружаем и классифицируем изображение
                    img = image.load_img(image_path, target_size=(224, 224))
                    x = image.img_to_array(img)
                    x = x / 255.0
                    x = x.reshape((1, 224, 224, 3))
                    pred = model.predict(x)

                    # определяем метку класса
                    if pred[0][0] > 0.5:
                        class_label = DIR_NAME_class01
                    else:
                        class_label = 'НЕ ' + str(DIR_NAME_class01)

                    # выводим результат в консоль
                    print(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    print(f'Training Accuracy: {history_2_2.history["accuracy"][-1]}')
                    print(f'Validation Accuracy: {history_2_2.history["val_accuracy"][-1]}\n')

                    # записываем результаты в файл
                    result_file.write(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    result_file.write(f'Training Accuracy: {history_2_2.history["accuracy"][-1]}\n')
                    result_file.write(f'Validation Accuracy: {history_2_2.history["val_accuracy"][-1]}\n')

# путь к директории с демонстрационными данными
demo_data_directory = f'/content/test_data/'

# путь к файлу, в который будут записаны результаты классификации
output_file_experiment_2_2 = '/content/test_results_experiment_2_2.txt'

# вызываем функцию для классификации
classify_images_in_directory(demo_data_directory, output_file_experiment_2_2)

# Изменение размера пакета не приводит к существенным изменениям итоговых показателей модели

# Эксперимент 3_1 Увеличение количества нейронов в слоях

# используем batch_size = 40
batch_size = 40

# создание модели
model = Sequential()
model.add(Conv2D(64, (3, 3), input_shape=(224, 224, 3))) # увеличиваем количество нейронов
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3))) # увеличиваем количество нейронов
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(256, (3, 3))) # увеличиваем количество нейронов
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# компилируем модель
model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(learning_rate=0.01, momentum=0.99),
    metrics=['accuracy']
)

# обучаем модель
num_epochs = 20
history_3_1 = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

# сохраняем модель
model.save('experiment_3_1_model.h5')

# создаем функцию classify_image
def classify_images_in_directory(directory_path, output_file):

    # открываем файл для записи результатов
    with open(output_file, 'w') as result_file:

        # получаем список поддиректорий (car и not_car)
        subdirectories = os.listdir(directory_path)

        for subdirectory in subdirectories:
            subdirectory_path = os.path.join(directory_path, subdirectory)

            # проверяем, является ли поддиректория директорией
            if os.path.isdir(subdirectory_path):

                # получаем список файлов в поддиректории
                image_files = os.listdir(subdirectory_path)

                for image_file in image_files:

                    # полный путь к изображению
                    image_path = os.path.join(subdirectory_path, image_file)

                    # загружаем и классифицируем изображение
                    img = image.load_img(image_path, target_size=(224, 224))
                    x = image.img_to_array(img)
                    x = x / 255.0
                    x = x.reshape((1, 224, 224, 3))
                    pred = model.predict(x)

                    # определяем метку класса
                    if pred[0][0] > 0.5:
                        class_label = DIR_NAME_class01
                    else:
                        class_label = 'НЕ ' + str(DIR_NAME_class01)

                    # выводим результат в консоль
                    print(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    print(f'Training Accuracy: {history_3_1.history["accuracy"][-1]}')
                    print(f'Validation Accuracy: {history_3_1.history["val_accuracy"][-1]}\n')

                    # записываем результаты в файл
                    result_file.write(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    result_file.write(f'Training Accuracy: {history_3_1.history["accuracy"][-1]}\n')
                    result_file.write(f'Validation Accuracy: {history_3_1.history["val_accuracy"][-1]}\n')

# путь к директории с демонстрационными данными
demo_data_directory = f'/content/test_data/'

# путь к файлу, в который будут записаны результаты классификации
output_file_experiment_3_1 = '/content/test_results_experiment_3_1.txt'

# вызываем функцию для классификации
classify_images_in_directory(demo_data_directory, output_file_experiment_3_1)

# Эксперимент 3_2 Уменьшение количества нейронов в слоях

# используем batch_size = 40
batch_size = 40

# создание модели
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(128)) # уменьшим количество нейронов в полносвязном слое
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# компиляция модели
model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(learning_rate=0.01, momentum=0.99),
    metrics=['accuracy']
)

# обучение модели
num_epochs = 20
history_3_2 = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

# сохраняем модель
model.save('experiment_3_2_model.h5')

# создаем функцию classify_image
def classify_images_in_directory(directory_path, output_file):

    # открываем файл для записи результатов
    with open(output_file, 'w') as result_file:

        # получаем список поддиректорий (car и not_car)
        subdirectories = os.listdir(directory_path)

        for subdirectory in subdirectories:
            subdirectory_path = os.path.join(directory_path, subdirectory)

            # проверяем, является ли поддиректория директорией
            if os.path.isdir(subdirectory_path):

                # получаем список файлов в поддиректории
                image_files = os.listdir(subdirectory_path)

                for image_file in image_files:

                    # полный путь к изображению
                    image_path = os.path.join(subdirectory_path, image_file)

                    # загружаем и классифицируем изображение
                    img = image.load_img(image_path, target_size=(224, 224))
                    x = image.img_to_array(img)
                    x = x / 255.0
                    x = x.reshape((1, 224, 224, 3))
                    pred = model.predict(x)

                    # определяем метку класса
                    if pred[0][0] > 0.5:
                        class_label = DIR_NAME_class01
                    else:
                        class_label = 'НЕ ' + str(DIR_NAME_class01)

                    # выводим результат в консоль
                    print(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    print(f'Training Accuracy: {history_3_2.history["accuracy"][-1]}')
                    print(f'Validation Accuracy: {history_3_2.history["val_accuracy"][-1]}\n')

                    # записываем результаты в файл
                    result_file.write(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    result_file.write(f'Training Accuracy: {history_3_2.history["accuracy"][-1]}\n')
                    result_file.write(f'Validation Accuracy: {history_3_2.history["val_accuracy"][-1]}\n')

# путь к директории с демонстрационными данными
demo_data_directory = f'/content/test_data/'

# путь к файлу, в который будут записаны результаты классификации
output_file_experiment_3_2 = '/content/test_results_experiment_3_2.txt'

# вызываем функцию для классификации
classify_images_in_directory(demo_data_directory, output_file_experiment_3_2)

# Увеличение количества нейронов в скрытых слоях приводит к улучшению итоговых показателей модели
# Уменьшение количества нейронов в полносвязном слое приводит к улучшению итоговых показателей модели

# Эксперимент 4_1 Изменение скорости обучения = 0.001, (изначально learning_rate=0.01)

# используем batch_size = 40
batch_size = 40

# создание модели
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# компиляция модели
model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(learning_rate=0.001, momentum=0.99),
    metrics=['accuracy']
)

# обучаем модель
num_epochs = 20
history_4_1 = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

# сохраняем модель
model.save('experiment_4_1_model.h5')

# создаем функцию classify_image
def classify_images_in_directory(directory_path, output_file):

    # открываем файл для записи результатов
    with open(output_file, 'w') as result_file:

        # получаем список поддиректорий (car и not_car)
        subdirectories = os.listdir(directory_path)

        for subdirectory in subdirectories:
            subdirectory_path = os.path.join(directory_path, subdirectory)

            # проверяем, является ли поддиректория директорией
            if os.path.isdir(subdirectory_path):

                # получаем список файлов в поддиректории
                image_files = os.listdir(subdirectory_path)

                for image_file in image_files:

                    # полный путь к изображению
                    image_path = os.path.join(subdirectory_path, image_file)

                    # загружаем и классифицируем изображение
                    img = image.load_img(image_path, target_size=(224, 224))
                    x = image.img_to_array(img)
                    x = x / 255.0
                    x = x.reshape((1, 224, 224, 3))
                    pred = model.predict(x)

                    # определяем метку класса
                    if pred[0][0] > 0.5:
                        class_label = DIR_NAME_class01
                    else:
                        class_label = 'НЕ ' + str(DIR_NAME_class01)

                    # выводим результат в консоль
                    print(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    print(f'Training Accuracy: {history_4_1.history["accuracy"][-1]}')
                    print(f'Validation Accuracy: {history_4_1.history["val_accuracy"][-1]}\n')

                    # записываем результаты в файл
                    result_file.write(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    result_file.write(f'Training Accuracy: {history_4_1.history["accuracy"][-1]}\n')
                    result_file.write(f'Validation Accuracy: {history_4_1.history["val_accuracy"][-1]}\n')

# путь к директории с демонстрационными данными
demo_data_directory = f'/content/test_data/'

# путь к файлу, в который будут записаны результаты классификации
output_file_experiment_4_1 = '/content/test_results_experiment_4_1.txt'

# вызываем функцию для классификации
classify_images_in_directory(demo_data_directory, output_file_experiment_4_1)

# Эксперимент 4_2 Изменение скорости обучения = 0.1, (изначально learning_rate=0.01)

# используем batch_size = 40
batch_size = 40

# создание модели
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# компиляция модели
model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(learning_rate=0.1, momentum=0.99),
    metrics=['accuracy']
)

# обучаем модель
num_epochs = 20
history_4_2 = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

# сохраняем модель
model.save('experiment_4_2_model.h5')

# создаем функцию classify_image
def classify_images_in_directory(directory_path, output_file):

    # открываем файл для записи результатов
    with open(output_file, 'w') as result_file:

        # получаем список поддиректорий (car и not_car)
        subdirectories = os.listdir(directory_path)

        for subdirectory in subdirectories:
            subdirectory_path = os.path.join(directory_path, subdirectory)

            # проверяем, является ли поддиректория директорией
            if os.path.isdir(subdirectory_path):

                # получаем список файлов в поддиректории
                image_files = os.listdir(subdirectory_path)

                for image_file in image_files:

                    # полный путь к изображению
                    image_path = os.path.join(subdirectory_path, image_file)

                    # загружаем и классифицируем изображение
                    img = image.load_img(image_path, target_size=(224, 224))
                    x = image.img_to_array(img)
                    x = x / 255.0
                    x = x.reshape((1, 224, 224, 3))
                    pred = model.predict(x)

                    # определяем метку класса
                    if pred[0][0] > 0.5:
                        class_label = DIR_NAME_class01
                    else:
                        class_label = 'НЕ ' + str(DIR_NAME_class01)

                    # выводим результат в консоль
                    print(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    print(f'Training Accuracy: {history_4_2.history["accuracy"][-1]}')
                    print(f'Validation Accuracy: {history_4_2.history["val_accuracy"][-1]}\n')

                    # записываем результаты в файл
                    result_file.write(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    result_file.write(f'Training Accuracy: {history_4_2.history["accuracy"][-1]}\n')
                    result_file.write(f'Validation Accuracy: {history_4_2.history["val_accuracy"][-1]}\n')

# путь к директории с демонстрационными данными
demo_data_directory = f'/content/test_data/'

# путь к файлу, в который будут записаны результаты классификации
output_file_experiment_4_2 = '/content/test_results_experiment_4_2.txt'

# вызываем функцию для классификации
classify_images_in_directory(demo_data_directory, output_file_experiment_4_2)

# Изменение скорости обучения = 0.001, относительно learning_rate = 0.1, демонстрирует улучшение показателей модели

# подбор best модели

# используем batch_size = 40
batch_size = 40

# создание модели
model = Sequential()
model.add(Conv2D(64, (3, 3), input_shape=(224, 224, 3))) # увеличим количество нейронов в скрытом слое (с 32 до 64)
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3))) # увеличим количество нейронов с 64 до 128
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(256, (3, 3))) # увеличим количество нейронов со 128 до 256
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(256)) # уменьшим количество нейронов в полносвязном слое (с 512 до 256)
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# компиляция модели
model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(learning_rate=0.01, momentum=0.99), # используем learning_rate=0.01
    metrics=['accuracy']
)

# обучаем модель
num_epochs = 50 # увеличим кол-во эпох (с 30 изначального варианта до 50)
history_best = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

# сохраняем модель
model.save('best_model.h5')

# создаем функцию classify_image
def classify_images_in_directory(directory_path, output_file):

    # открываем файл для записи результатов
    with open(output_file, 'w') as result_file:

        # получаем список поддиректорий (car и not_car)
        subdirectories = os.listdir(directory_path)

        for subdirectory in subdirectories:
            subdirectory_path = os.path.join(directory_path, subdirectory)

            # проверяем, является ли поддиректория директорией
            if os.path.isdir(subdirectory_path):

                # получаем список файлов в поддиректории
                image_files = os.listdir(subdirectory_path)

                for image_file in image_files:

                    # полный путь к изображению
                    image_path = os.path.join(subdirectory_path, image_file)

                    # загружаем и классифицируем изображение
                    img = image.load_img(image_path, target_size=(224, 224))
                    x = image.img_to_array(img)
                    x = x / 255.0
                    x = x.reshape((1, 224, 224, 3))
                    pred = model.predict(x)

                    # определяем метку класса
                    if pred[0][0] > 0.5:
                        class_label = DIR_NAME_class01
                    else:
                        class_label = 'НЕ ' + str(DIR_NAME_class01)

                    # выводим результат в консоль
                    print(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    print(f'Training Accuracy: {history_best.history["accuracy"][-1]}')
                    print(f'Validation Accuracy: {history_best.history["val_accuracy"][-1]}\n')

                    # записываем результаты в файл
                    result_file.write(f'Image: {os.path.join(subdirectory, image_file)}, Prediction: {class_label}\n')
                    result_file.write(f'Training Accuracy: {history_best.history["accuracy"][-1]}\n')
                    result_file.write(f'Validation Accuracy: {history_best.history["val_accuracy"][-1]}\n')

# путь к директории с демонстрационными данными
demo_data_directory = f'/content/test_data/'

# путь к файлу, в который будут записаны результаты классификации
output_file_best = '/content/test_results_experiment_best.txt'

# вызываем функцию для классификации
classify_images_in_directory(demo_data_directory, output_file_best)

# Таким образом, проведя несколько проверок параметров модели, были определены следующие оптимальные показатели для используемого датасета:
# количество эпох обучения = 50;
# batch_size = 40;
# увеличение количества нейронов в скрытых слоях и уменьшение количества нейронов в полносвязном слое;
# learning_rate=0.01.