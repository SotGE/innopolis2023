# -*- coding: utf-8 -*-
"""exam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hMPW8lfmeuqPYqFc0jSy_gpjgVFBLkR9
"""

!pip install tabulate
!pip install texttable
!pip install pymorphy3
!pip install catboost
!pip install xgboost
!pip install progressbar2
!pip install tsfresh
!pip install -U imbalanced-learn
!pip install eli5
!pip install openpyxl

import re
import time
import datetime
import csv
import requests
import joblib
import random

# python -m pip install pandas tabulate
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup # для парсера HTML
import openpyxl # для парсера XLSX (Excel)

import pymorphy3 # работа с русским языком, pymorphy3

import texttable as tt

import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder # Кодирование категориальных данных

from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler # Масштабирование данных

from sklearn.feature_selection import SelectKBest # Выбор признаков с наивысшими оценками
from sklearn.feature_selection import chi2 # Выбор признаков по Хи квадрат

from sklearn.model_selection import train_test_split # Деление выборки на тестовые и тренировочные данные
from sklearn.model_selection import cross_val_score # Оценка качества работы модели

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error, classification_report # Критерий качества, точности

# from sklearn.neighbors import KNeighborsClassifier # Обучение модели K-ближайших соседей
# from sklearn.ensemble import BaggingClassifier # Комплексный метаоценщик
# from sklearn.ensemble import RandomForestClassifier # Ансамбли деревьев решений
# from sklearn.ensemble import GradientBoostingClassifier # Ансамбли градиентного спуска (повышения градиента)
# from sklearn.ensemble import ExtraTreesClassifier # Дополнительные деревья
# from sklearn.ensemble import AdaBoostClassifier # Алгоритм усиления ансамбля
# from sklearn.ensemble import VotingClassifier # Стэккинг (способ объединения прогнозов)
# from sklearn.tree import DecisionTreeClassifier # Деревья решений
# from catboost import CatBoostClassifier
# from xgboost import XGBClassifier

from sklearn.ensemble import BaggingRegressor # Комплексный метаоценщик
from sklearn.ensemble import GradientBoostingRegressor # Ансамбли градиентного спуска (повышения градиента)
from sklearn.ensemble import StackingRegressor
from sklearn.ensemble import VotingRegressor # Стэккинг (способ объединения прогнозов)
from sklearn.ensemble import AdaBoostRegressor # Алгоритм усиления ансамбля
from sklearn.ensemble import ExtraTreesRegressor # Дополнительные деревья
from sklearn.ensemble import RandomForestRegressor # Ансамбли деревьев решений (случайный лес)
from sklearn.tree import DecisionTreeRegressor # Деревья решений
from sklearn.linear_model import LinearRegression # Линейная регрессия (метод наименьших квадратов)
from sklearn.neighbors import KNeighborsRegressor # Обучение модели K-ближайших соседей
from sklearn.svm import SVR # метод опорных векторов с линейным ядром
from sklearn.linear_model import LogisticRegression # логистическая регрессия

# python -m pip install catboost
from catboost import CatBoostRegressor

# python -m pip install xgboost
from xgboost import XGBRegressor

from sklearn.feature_selection import GenericUnivariateSelect, mutual_info_classif # Статистический метод
from sklearn.metrics import r2_score

# python -m pip install mlxtend
from mlxtend.plotting import plot_decision_regions

# Feature engineering:
# python -m pip install progressbar2
import progressbar

# python -m pip install tsfresh
from tsfresh.examples.robot_execution_failures import download_robot_execution_failures, load_robot_execution_failures
from tsfresh.examples.har_dataset import download_har_dataset, load_har_dataset, load_har_classes
from tsfresh import extract_features, extract_relevant_features, select_features
from tsfresh.utilities.dataframe_functions import impute
from tsfresh.utilities.dataframe_functions import roll_time_series
from tsfresh.feature_extraction import settings
from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters, ComprehensiveFCParameters

# python -m pip install -U imbalanced-learn
# python -m pip install eli5
# import eli5
# from eli5.sklearn import PermutationImportance

"""# Подготавливаю данные о погоде в "село Сембург", близлежайшее поселение к нефтянной скважины №807 (в 24 км). Разрабатываю парсер сайтов о погоде."""

# проведу осмотр данных прогноза погоды в с. Самбург на сайте Яндекс.Погода за текущий год (на данный момент это 2023 год)
# lat=66.80035786146756 (Широта)
# lng=78.38975066524623 (Долгота)

month = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']

morph = pymorphy3.MorphAnalyzer(lang='ru')

data = []
data_i = 0
for j, item in enumerate(month):
  url = 'https://yandex.ru/pogoda/month/{item}?lat={latitude}&lon={lontitude}'
  url = url.format(item=item, latitude=66.80035786146756, lontitude=78.38975066524623)
  print(url)
  page = requests.get(url)
  bs = BeautifulSoup(page.text, 'html.parser')

  # информация о погоде в с. Самбург за каждый месяц
  for i, val in enumerate(bs.find('table', {'class': 'climate-calendar'}).find_all('td', {'class': 'climate-calendar__cell'})):
    temp = []
    if val.find('div', {'class': 'climate-calendar-day_colorless_yes'}):
      continue
    temp.append(int(data_i))
    temp.append(int(val.find('div', {'class': 'climate-calendar-day__detailed-container-center'}).find_next('h6').text.split(',', 1)[0].split(' ', 1)[0]))
    temp.append(str(morph.parse(val.find('div', {'class': 'climate-calendar-day__detailed-container-center'}).find_next('h6').text.split(',', 1)[0].split(' ', 1)[1])[0].normal_form))
    temp.append(2023)
    temp.append(str(val.find('div', {'class': 'climate-calendar-day__detailed-container-center'}).find_next('h6').text.split(',', 1)[1]))
    temp.append(int(val.find('div', {'class': 'temp climate-calendar-day__detailed-basic-temp-day'}).find('span').text.replace('−', '-')))
    temp.append(int(val.find('div', {'class': 'temp climate-calendar-day__detailed-basic-temp-night'}).find('span').text.replace('−', '-')))
    temp.append(int(val.find_all('td', {'class': 'climate-calendar-day__detailed-data-table-cell climate-calendar-day__detailed-data-table-cell_value_yes'})[0].text.split(' ', 1)[0]))
    temp.append(int(val.find_all('td', {'class': 'climate-calendar-day__detailed-data-table-cell climate-calendar-day__detailed-data-table-cell_value_yes'})[1].text.split('%', 1)[0]) / 100)
    temp.append(float(val.find_all('td', {'class': 'climate-calendar-day__detailed-data-table-cell climate-calendar-day__detailed-data-table-cell_value_yes'})[2].find('div').text))
    temp.append(str(val.find_all('td', {'class': 'climate-calendar-day__detailed-data-table-cell climate-calendar-day__detailed-data-table-cell_value_yes'})[2].find('abbr').text))
    print(temp)
    data.append(temp)
    data_i += 1

# Датафрейм из массива
df = pd.DataFrame(data, columns=['id', 'День', 'Месяц', 'Год', 'День недели', 'Температура днем', 'Температура ночью', 'Давление (мм рт. ст.)', 'Влажность (%)', 'Скорость ветра', 'Направление ветра'])
df

# cоздаю объект Texttable
tab = tt.Texttable()

# устанавливаю стили таблицы
tab.set_cols_align(['c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c'])
tab.header(['id', 'День', 'Месяц', 'Год', 'День недели', 'Температура днем', 'Температура ночью', 'Давление (мм рт. ст.)', 'Влажность (%)', 'Скорость ветра', 'Направление ветра'])

# преобразую DataFrame в список списков (двумерный список)
data_list = df.values.tolist()

# добавляю данные в таблицу
for row in data_list:
  tab.add_row(row)

# получаю отформатированную таблицу в виде строки
table_string = tab.draw()

# вывожу таблицу на экран
print(table_string)

"""# Как видно я получил данные с Яндекс.Погода за 2023 года, мне нужно больше данных. У Яндекс.Погода нет архивных данных, поэтому буду использовать другой сайт с архивными данными."""

# рассмотрю данные прогноза погоды в с. Самбург с сайта Погода1 (так как в нем есть архивные данные)
url = 'https://pogoda1.ru/samburg/arkhiv/'
page = requests.get(url)
bs = BeautifulSoup(page.text, 'html.parser')

# Формирую список диапозона даты, где имеются архивные данные
year = bs.find('select', {'class': 'select-archive-year'}).find_all('option')
month = bs.find('select', {'class': 'select-archive-month'}).find_all('option')

# Перевод месяцев с русского на английский язык
def month_translate(month):
  month = month.lower()
  if month == "январь":
    return "january"
  elif month == "февраль":
    return "february"
  elif month == "март":
    return "march"
  elif month == "апрель":
    return "april"
  elif month == "май":
    return "may"
  elif month == "июнь":
    return "june"
  elif month == "июль":
    return "july"
  elif month == "август":
    return "august"
  elif month == "сентябрь":
    return "september"
  elif month == "октябрь":
    return "october"
  elif month == "ноябрь":
    return "november"
  elif month == "декабрь":
    return "december"

base_month = []
for i, val_year in enumerate(year):
  for j, val_month in enumerate(month):
    temp_0 = []
    if j > 0:
      temp_0.append(int(val_year.text))
      temp_0.append(str(month_translate(month = val_month.text)).lower())
      base_month.append(temp_0)

# Формирую список URL адресов с данными
url_parse = 'https://pogoda1.ru/samburg/{month}-{year}/'

for i, val in enumerate(base_month):
  parse = url_parse.format(month=val[1], year=val[0])
  base_month[i].append(parse)

df = pd.DataFrame(base_month, columns=['Год', 'Месяц', 'URL'])
df

# Перевод месяцев с текста на цифры
def month_number(month):
  month = month.lower()
  if month == "january":
    return "01"
  elif month == "february":
    return "02"
  elif month == "march":
    return "03"
  elif month == "april":
    return "04"
  elif month == "may":
    return "05"
  elif month == "june":
    return "06"
  elif month == "july":
    return "07"
  elif month == "august":
    return "08"
  elif month == "september":
    return "09"
  elif month == "october":
    return "10"
  elif month == "november":
    return "11"
  elif month == "december":
    return "12"

# Поиск существующих ссылок с данными
base_day = []
for i, val in enumerate(base_month):
  bs = BeautifulSoup(requests.get(val[2]).text, 'html.parser')
  if (bs.find('div', {'class': 'month-calendar calendar'})):
    print(val[2])
    for j, item in enumerate(bs.find_all('a', {'class': 'calendar-item'})):
      temp = []
      if not (item.find('span', {'class': 'no-data'})):
        temp.append(int(val[0]))
        temp.append(int(month_number(val[1])))
        temp.append(int(item.find('span', {'month-calendar-day'}).text))
        temp.append('https://pogoda1.ru' + str(item['href']))
        print(temp)
        base_day.append(temp)

# Парсер погоды
data = []
for i, val in enumerate(base_day):
  # if i < 1593:
  #   continue
  temp = []
  bs = BeautifulSoup(requests.get(val[3]).text, 'html.parser')
  if not (bs.find('div', {'class': 'panel-heading'}).text == '404 Страница не найдена'):
    # print(val[3])
    temp.append(i)
    temp.append(val[0])
    temp.append(val[1])
    temp.append(val[2])
    temp.append(str(bs.find('img', {'class': 'weather-now-icon-img'})['title']).lower())
    if (bs.find('div', {'class': 'weather-now-col weather-now-col-main'}).find('span', {'class': 'wind-amount'})):
      temp.append(str(bs.find_all('span', {'class': 'wind-amount'})[0].text.split(',', 1)[0]).lower())
      temp.append(int(bs.find_all('span', {'class': 'wind-amount'})[0].text.split(',', 1)[1].split(' ', 2)[1]))
    else:
      for j, item in enumerate(bs.find_all('div', {'row-forecast-time-of-day'})):
        if not (str(item.find('div', {'class': 'cell-forecast-wind'}).find('span', {'class': 'wind'}).text) == 'нет'):
          temp.append(str(item.find('div', {'class': 'cell-forecast-wind'}).find('img', {'class': 'icon-wind'})['title'].split(' ', 1)[0]).lower())
          temp.append(int(item.find('div', {'class': 'cell-forecast-wind'}).find('span', {'class': 'wind-amount'}).text.split(' ', 1)[0]))
        else:
          temp.append('')
          temp.append('')
        break
    temp.append(int(bs.find_all('div', {'class': 'weather-now-info'})[0].find('span', {'class': 'value'}).text.split(' ', 1)[0]))
    temp.append(int(bs.find_all('div', {'class': 'weather-now-info'})[1].find('span', {'class': 'value'}).text.split('%', 1)[0]))
    temp.append(float(bs.find_all('div', {'class': 'weather-now-info'})[2].find('span', {'class': 'value'}).text.split(' ', 1)[0]))
    temp.append(str(bs.find_all('div', {'class': 'weather-now-info'})[6].find('span', {'class': 'value'}).text.split(' ', 1)[0]).lower())
    # temp.append(int(bs.find_all('div', {'class': 'weather-now-info'})[6].find('span', {'class': 'value'}).text.split(' ', 1)[1].split('%', 1)[0]))
    if not (str(bs.find_all('div', {'row-forecast-time-of-day'})[2].find('div', {'class': 'cell-forecast-prec opened'}))):
      if (str(bs.find_all('div', {'row-forecast-time-of-day'})[2].find('div', {'class': 'cell-forecast-prec'}).text) == 'без осадков'):
        temp.append(0)
      else:
        temp.append(float(bs.find_all('div', {'row-forecast-time-of-day'})[2].find('div', {'class': 'cell-forecast-prec'}).text.split(' ', 1)[0]))
    else:
      if (str(bs.find_all('div', {'row-forecast-time-of-day'})[1].find('div', {'class': 'cell-forecast-prec'}).text) == 'без осадков'):
        temp.append(0)
      else:
        temp.append(float(bs.find_all('div', {'row-forecast-time-of-day'})[1].find('div', {'class': 'cell-forecast-prec'}).text.split(' ', 1)[0]))
    temp.append(int(bs.find_all('div', {'row-forecast-time-of-day'})[0].find('div', {'class': 'cell-forecast-temp'}).text.split('°', 1)[0]))
    if not (str(bs.find_all('div', {'row-forecast-time-of-day'})[2].find('div', {'class': 'cell-forecast-prec opened'}))):
      temp.append(int(bs.find_all('div', {'row-forecast-time-of-day'})[2].find('div', {'class': 'cell-forecast-temp'}).text.split('°', 1)[0]))
    else:
      temp.append(int(bs.find_all('div', {'row-forecast-time-of-day'})[1].find('div', {'class': 'cell-forecast-temp'}).text.split('°', 1)[0]))
    temp.append(val[3])
    print(temp)
    data.append(temp)

# Оформляю в DataFrame
df = pd.DataFrame(data, columns=[
  'id',
  'год',
  'месяц',
  'день',
  'погодное условие',
  'направление ветра',
  'скорость ветра (м/с)',
  'давление (мм рт. ст.)',
  'влажность (%)',
  'видимость (мм)',
  'луна',
  'осадки (мм)',
  'температура днем',
  'температура ночью',
  'url'
])
# df.drop(columns=["id"], inplace=True)

# Сохранение данных в csv
df.to_csv('/content/data_samburg_weather.csv', sep=',', encoding='utf-8', index=False)
df

"""# Необходимые данные получил в размере 2356 строк. Сохранил данные в файл и загрузил к себе в Git репозиторий. Дальше идет подготовка данных, а именно просмотр пустот и заполнения средними или наиболее встречающимися значениями, просмотр типов данных."""

# Загрузка файла из Git моего репозитория в Pandas
# data_samburg_weather = pd.read_csv('https://raw.githubusercontent.com/SotGE/innopolis2023/main/exam/data_samburg_weather.csv', sep=',', index_col=False, quoting=csv.QUOTE_MINIMAL)
data_samburg_weather = pd.read_csv('https://raw.githubusercontent.com/SotGE/innopolis2023/main/exam/data_samburg_weather.csv', sep=',', index_col=False)
data_samburg_weather

# Размер данных (количество строк, колонок)
data_samburg_weather.shape

# Просмотр типов данных в датасете
print(data_samburg_weather.info())

# Проверка данных

# Количество пустых ячеек
data_samburg_weather.isnull().sum()

# Количество неопределенные значений (неправильно считанные)
data_samburg_weather.isna().sum()

# Колличество пустых строк
(data_samburg_weather == "").sum()

# Заполнение данных

# Заполнение пустых значений - наиболее встречающимся классом
# df['направление ветра'] = df['направление ветра'].replace('', str(df['направление ветра'].value_counts().index[0]))
data_samburg_weather['направление ветра'].fillna(str(data_samburg_weather['направление ветра'].value_counts().index[0]), inplace = True)

# Заполнение пустых значений - наиболее распространенного значения
# data_samburg_weather['скорость ветра (м/с)'] = data_samburg_weather['скорость ветра (м/с)'].replace('', float(data_samburg_weather['скорость ветра (м/с)'].value_counts().idxmax()))
data_samburg_weather['скорость ветра (м/с)'].fillna(float(data_samburg_weather['скорость ветра (м/с)'].value_counts().idxmax()), inplace = True)

data_samburg_weather = data_samburg_weather.astype({'скорость ветра (м/с)': 'float64'})

# Просмотр типов данных в датасете
print(data_samburg_weather.info())

# Повторная проверка данных

# Количество пустых ячеек
data_samburg_weather.isnull().sum()

# Количество неопределенные значений (неправильно считанные)
data_samburg_weather.isna().sum()

# Колличество пустых строк
(data_samburg_weather == '').sum()

# Описательная статистика
data_samburg_weather.describe(include='all', percentiles=[0.1, 0.25,0.5, 0.75, 0.9]).T

"""# На этом парсер погоды закончен, перехожу к подготовке датасета о нефтянной скважины №807 (данные хранятся в формате xlsx, их переведу в формат csv)."""

# загружаю xlsx документ и выполняю парсер
wookbook = openpyxl.load_workbook("/content/Oil well.xlsx")
worksheet = wookbook["Oil Well"]
data_xlsx = []
data_i = 0
for row in worksheet.iter_rows(min_row=4, min_col=1, max_col=9):
  temp = []
  temp.append(data_i)
  for j, cell in enumerate(row):
    if j == 0:
      temp.append(cell.value.year)
      temp.append(cell.value.month)
      temp.append(cell.value.day)
    else:
      temp.append(cell.value)
  data_xlsx.append(temp)
  data_i += 1

df = pd.DataFrame(data_xlsx, columns=[
  'id',
  'год',
  'месяц',
  'день',
  'объем нефти (м3/сутки)',
  'объем жидкости (м3/сутки)',
  'объем газа (м3/сутки)',
  'объем воды (м3/сутки)',
  'обводненность (%)',
  'рабочее время',
  'динамический уровень (м)',
  'пластовое давление (атм)'
])
# df.drop(columns=["id"], inplace=True)

# Сохранение данных в csv
df.to_csv('data_oil_well_807.csv', sep=',', encoding='utf-8', index=False)
df

"""# Необходимые данные получил в размере 2939 строк. Сохранил данные в файл и загрузил к себе в Git репозиторий. Дальше идет подготовка данных, а именно просмотр пустот и заполнения средними или наиболее встречающимися значениями, просмотр типов данных."""

# Загрузка файла из Git моего репозитория в Pandas
# data_oil_well_807 = pWd.read_csv('https://raw.githubusercontent.com/SotGE/innopolis2023/main/exam/data_oil_well_807.csv', sep=',', index_col=False, quoting=csv.QUOTE_MINIMAL)
data_oil_well_807 = pd.read_csv('https://raw.githubusercontent.com/SotGE/innopolis2023/main/exam/data_oil_well_807.csv', sep=',', index_col=False)
data_oil_well_807

# Размер данных (количество строк, колонок)
data_oil_well_807.shape

# Просмотр типов данных в датасете
print(data_oil_well_807.info())

# Проверка данных

# Количество пустых ячеек
data_oil_well_807.isnull().sum()

# Количество неопределенные значений (неправильно считанные)
data_oil_well_807.isna().sum()

# Колличество пустых строк
(data_oil_well_807 == '').sum()

# Описательная статистика
data_oil_well_807.describe(include='all', percentiles=[0.1, 0.25,0.5, 0.75, 0.9]).T

"""# На этом парсер нефтянной скважины №807 закончен. Теперь есть 2 датасета: data_samburg_weather.csv и data_oil_well_807.csv"""

# найду общие даты между ними и обрежу датасеты (объеденение 2х датасетов в один)
# data_samburg_weather
# data_oil_well_807

data = data_samburg_weather.merge(data_oil_well_807, how='inner', on=['год', 'месяц', 'день'])
data

# удалю ненужные столбцы
data.drop(columns=[
  'id_x',
  'url',
  'id_y'
], inplace=True)
data

"""# Данные для исседования подготовил."""

# Проверка значений на 0
(data == 0).sum()

# Как видно, представленные параметры могут находится в значении 0

# Заполнение нулевых значений - медианой не требуется
# data = data.replace(0, data.median())

# Просмотр типов данных в датасете
data.info()

# Перевел категориальные данные - строковые в числовые с сортировкой по алфавиту
data['погодное условие'] = pd.factorize(data['погодное условие'].astype("category"), sort=True)[0]+1
data['направление ветра'] = pd.factorize(data['направление ветра'].astype("category"), sort=True)[0]+1
data['луна'] = pd.factorize(data['луна'].astype("category"), sort=True)[0]+1
data

# Просмотр типов данных в датасете
data.info()

# Получил все числовые данные

# Построить распределение для всех числовых переменных
figure = px.box(data)
figure.show()

# рассчитаю коэффициенты корреляции для всех столбцов (посмотрю не связаны ли между собой какие-либо атрибуты)
data.corr()

# посмотрю пары с высокой корреляцией
df = data.corr().abs().unstack().sort_values(ascending = False).drop_duplicates()

# сброс ограничений на вывод табличных данных
# pd.set_option('display.max_rows', None)
# pd.set_option('display.max_columns', None)
# pd.set_option('display.max_colwidth', None)
# df

print(df.to_markdown())

# display(df.to_string())

# наглядный график с высокой корреляцией
dataCorr = data.corr()
filteredDf = dataCorr[(abs(dataCorr) >= .1) & (dataCorr != 1.000)]
plt.figure(figsize=(30, 10))
sns.heatmap(abs(filteredDf), annot=True, cmap="Reds")
plt.show()

"""# Я не учитываю в корреляции ['погодное условие', 'направление ветра', 'луна'] так как это категориальные данные, переведенные в число."""

# Разделение для задачи классификации на X (экзогенные переменные, т.е. регрессоры или независимые) и y (эндогенные переменные или зависимые)

# ['
#   'погодное условие',
#   'направление ветра',
#   'скорость ветра (м/с)',
#   'давление (мм рт. ст.)',
#   'влажность (%)',
#   'видимость (мм)',
#   'луна',
#   'осадки (мм)',
#   'температура днем',
#   'температура ночью'
# ]

# ['
#   'объем нефти (м3/сутки)',
#   'объем жидкости (м3/сутки)',
#   'объем газа (м3/сутки)',
#   'объем воды (м3/сутки)',
#   'обводненность (%)',
#   'рабочее время',
#   'динамический уровень (м)',
#   'пластовое давление (атм)'
# ]

X = data.drop([
  'объем нефти (м3/сутки)',
  'объем жидкости (м3/сутки)',
  'объем газа (м3/сутки)',
  'объем воды (м3/сутки)',
  'обводненность (%)',
  'рабочее время',
  'динамический уровень (м)',
  'пластовое давление (атм)'
], axis=1)

y = data[[
  'объем нефти (м3/сутки)',
  'объем жидкости (м3/сутки)',
  'объем газа (м3/сутки)',
  'объем воды (м3/сутки)',
  'обводненность (%)',
  'рабочее время',
  'динамический уровень (м)',
  'пластовое давление (атм)'
]]

X_original_1 = X
y_original_1 = y

# Уберу не нужные данные с датой, так как я не буду исследовать данные в зависимости от времени года
X = X.drop(['год', 'месяц', 'день'], axis=1)

# Подготовка данных

# Нормализация (MinMaxScaler)
scalar = MinMaxScaler()
features = scalar.fit_transform(X, y)
X_normalised = pd.DataFrame(features, columns=X.columns)
X_normalised

# Построить распределение для всех числовых нормализированных переменных
figure = px.box(X_normalised)
figure.show()

# Разделение на тренировочную (80%), тестовую (10%) и валидационную (10%)
X_train, X_test, y_train, y_test = train_test_split(X_normalised, y, test_size=0.2, random_state=1024, shuffle=True)
X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1024, shuffle=True)

# Рассмотрю разные алгоритмы
models = [
  LinearRegression(), # Метод наименьших квадратов
  RandomForestRegressor(n_estimators=100, max_features ='sqrt'), # Случайный лес
  KNeighborsRegressor(n_neighbors=6), # Метод ближайших соседей
  # SVR(kernel='linear'), # Метод опорных векторов с линейным ядром (для одномерного массива y)
  # LogisticRegression() # Логистическая регрессия (для одномерного массива y)
  # DecisionTreeClassifier(max_depth=4, random_state=42), # Деревья решений
  # RandomForestClassifier(min_samples_split=5, n_estimators=1000), # Ансамбли деревьев решений
  # GradientBoostingClassifier(max_depth=1, n_estimators=1000), # Ансамбли градиентного спуска
  # KNeighborsClassifier(n_neighbors=5), # Обучение модели K-ближайших соседей
]

# Строю графики тренировочных данных
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))
sns.histplot(data=X_train, ax=axes[0], palette='dark')
boxplot = sns.boxplot(data=X_train, ax=axes[1], palette='pastel')
boxplot.tick_params(axis='x', rotation=30)
plt.tight_layout()

# Создаю временную структуру для графика
models_test = pd.DataFrame()
models_temp = {}

# Для каждой модели из списка
for model in models:
  # Для каждого столбцам результирующего набора
  for i in range(y_train.shape[1]):
    # Обучаю модель
    model.fit(X_train, y_train.iloc[:, i])
    print(f"\nМодель {type(model).__name__}:")
    print("Правильность на обучающем наборе: {:.3f}".format(model.score(X_train, y_train.iloc[:, i])))
    print("Правильность на тестовом наборе: {:.3f}".format(model.score(X_test, y_test.iloc[:, i])))
    # Коэффициент детерминации
    models_temp[f'R2_y{i + 1}'] = r2_score(y_test.iloc[:, 0], model.predict(X_test))
  models_test = pd.concat([models_test, pd.DataFrame([models_temp])], ignore_index=True)

# Коэффициенты детерминации
models_test_abs = models_test.abs()
models_test_abs

# Масштабирую данные о 0 до 1 относительно каждого столбца
models_test_normalized = (models_test_abs-models_test_abs.min()) / (models_test_abs.max()-models_test_abs.min())
models_test_normalized

# Строю графики с коэффициентами детерминации
fig, axes = plt.subplots(ncols=y_train.shape[1], figsize=(12, 6))

for i in range(y_train.shape[1]):
  models_test_normalized[f'R2_y{i + 1}'].plot(ax=axes[i], kind='bar', title=f'R2_y{i + 1}')

# y1 = 'объем нефти (м3/сутки)',
# y2 = 'объем жидкости (м3/сутки)',
# y3 = 'объем газа (м3/сутки)',
# y4 = 'объем воды (м3/сутки)',
# y5 = 'обводненность (%)',
# y6 = 'рабочее время',
# y7 = 'динамический уровень (м)',
# y8 = 'пластовое давление (атм)'

# Как видно из графика:

# Метод наименьших квадратов (LinearRegression) подходит в большей степени для:
#   y5 = 'обводненность (%)',

# Случайный лес (RandomForestRegressor) подходит в большей степени для:
#   y3 = 'объем газа (м3/сутки)',
#   y4 = 'объем воды (м3/сутки)',
#   y7 = 'динамический уровень (м)',
#   y8 = 'пластовое давление (атм)'

# Метод ближайших соседей (KNeighborsRegressor) подходит в большей степени для:
#   y1 = 'объем нефти (м3/сутки)',
#   y2 = 'объем жидкости (м3/сутки)',
#   y4 = 'объем воды (м3/сутки)',
#   y5 = 'обводненность (%)',
#   y6 = 'рабочее время',

# Проведу анализ лучших признаков

# Для каждого столбца результирующего набора
for i in range(y_train.shape[1]):
  # Выбор 2 лучших признаков из 8 по Хи квадрат
  selector = SelectKBest(chi2, k=2)
  X_train_best = selector.fit_transform(X_train, y_train.iloc[:, i])
  # print(X_train_best.shape)
  X.columns[selector.get_support(indices=True)]
  vector_names = list(X.columns[selector.get_support(indices=True)])
  print(f'Для {[y.columns[i]]} лучшие признаки: {vector_names}')
  # X_train_best_df = pd.DataFrame(X_train_best, columns=selector.get_support(indices=True))
  # print(X_train_best_df)

# обучение лучшей модели с лучшими признаками
model_best = models[2]
model_best.fit(X_train, y_train)

# качество модели
score = model.score(X_test,y_test)
print("Accuracy: ", score*100) # A, P, R, E

# предсказание на тестовой выборке
data_best = pd.DataFrame(model_best.predict(X_test), columns=[
  'объем нефти (м3/сутки)',
  'объем жидкости (м3/сутки)',
  'объем газа (м3/сутки)',
  'объем воды (м3/сутки)',
  'обводненность (%)',
  'рабочее время',
  'динамический уровень (м)',
  'пластовое давление (атм)'
])
data_best

# score модели (метрики для классификации)
def model_report(model, X_train, y_train, X_test, y_test, average='weighted'):
  # Делаю предсказания на тренировочном наборе
  y_pred_train = model.predict(X_train)
  # Делаю предсказания на тестовом наборе
  y_pred_test = model.predict(X_test)

  # Оцениваю точность модели на тренировочном наборе
  print(f"Тренировочный набор - Матрица ошибок (confusion matrix) модели:\n{confusion_matrix(y_train, y_pred_train)}")
  print(f"Тренировочный набор - Правильность (accuracy) модели: {accuracy_score(y_train, y_pred_train)}")
  print(f"Тренировочный набор - Точность (precision) модели: {precision_score(y_train, y_pred_train, average=average)}")
  print(f"Тренировочный набор - Полнота (recall) модели: {recall_score(y_train, y_pred_train, average=average)}")
  print(f"Тренировочный набор - F1 мера модели: {f1_score(y_train, y_pred_train, average=average)}")
  print(f"Тренировочный набор - Средняя абсолютная ошибка (mean absolute error): {mean_absolute_error(y_train, y_pred_train)}\n\n")

  # Оцениваю точность модели на тестовом наборе
  print(f"Тестовый набор - Матрица ошибок (confusion matrix) модели:\n{confusion_matrix(y_test, y_pred_test)}")
  print(f"Тестовый набор - Правильность (accuracy) модели: {accuracy_score(y_test, y_pred_test)}")
  print(f"Тестовый набор - Точность (precision) модели: {precision_score(y_test, y_pred_test, average=average)}")
  print(f"Тестовый набор - Полнота (recall) модели: {recall_score(y_test, y_pred_test, average=average)}")
  print(f"Тестовый набор - F1 мера модели: {f1_score(y_test, y_pred_test, average=average)}")
  print(f"Тестовый набор - Средняя абсолютная ошибка (mean absolute error): {mean_absolute_error(y_test, y_pred_test)}\n\n")

# Сделаю обучение (KNeighborsRegressor) влияние температуры на обводненность
KNeighbors = KNeighborsRegressor(n_neighbors=6)
KNeighbors.fit(X_train[['температура днем', 'температура ночью']], y_train[['обводненность (%)']])
print(f"Тренировочный набор: {KNeighbors.score(X_train[['температура днем', 'температура ночью']], y_train[['обводненность (%)']])*100}")
print(f"Тестовый набор: {KNeighbors.score(X_test[['температура днем', 'температура ночью']], y_test[['обводненность (%)']])*100}")

# Сделаю обучение (KNeighborsRegressor) влияние температуры на рабочее время
KNeighbors = KNeighborsRegressor(n_neighbors=6)
KNeighbors.fit(X_train[['температура днем', 'температура ночью']], y_train[['рабочее время']])
print(f"Тренировочный набор - {KNeighbors.score(X_train[['температура днем', 'температура ночью']], y_train[['рабочее время']])*100}")
print(f"Тестовый набор - {KNeighbors.score(X_test[['температура днем', 'температура ночью']], y_test[['рабочее время']])*100}")

# Сделаю обучение (KNeighborsRegressor) влияние температуры на динамический уровень
KNeighbors = KNeighborsRegressor(n_neighbors=6)
KNeighbors.fit(X_train[['температура днем', 'температура ночью']], y_train[['динамический уровень (м)']])
print(f"Тренировочный набор - {KNeighbors.score(X_train[['температура днем', 'температура ночью']], y_train[['динамический уровень (м)']])*100}")
print(f"Тестовый набор - {KNeighbors.score(X_test[['температура днем', 'температура ночью']], y_test[['динамический уровень (м)']])*100}")

# Сделаю обучение (KNeighborsRegressor) влияние температуры на пластовое давление
KNeighbors = KNeighborsRegressor(n_neighbors=6)
KNeighbors.fit(X_train[['температура днем', 'температура ночью']], y_train[['пластовое давление (атм)']])
print(f"Тренировочный набор - {KNeighbors.score(X_train[['температура днем', 'температура ночью']], y_train[['пластовое давление (атм)']])*100}")
print(f"Тестовый набор - {KNeighbors.score(X_test[['температура днем', 'температура ночью']], y_test[['пластовое давление (атм)']])*100}")

"""# Как видно, показатели получены плохие. Исходя из логического мышления предметной области. Искусственное поднятие пластового давления зависит от закачки воды. Естественное поднятие пластового давления зависит от водоносного слоя, который подпирает нефть, что приводит к увеличению давления. Сам же водоносный слой зависит от осадков, чем больше остадков, тем выше водоносный слой."""

# Сгруппирую данные по дням, месяцам и годам (среднее)
dataml_group = data.groupby(by=[data['месяц'], data['год']]).mean()

# Разделение для задачи классификации на X (экзогенные переменные, т.е. регрессоры или независимые) и y (эндогенные переменные или зависимые)
X = dataml_group.drop([
  'объем нефти (м3/сутки)',
  'объем жидкости (м3/сутки)',
  'объем газа (м3/сутки)',
  'объем воды (м3/сутки)',
  'обводненность (%)',
  'рабочее время',
  'динамический уровень (м)',
  'пластовое давление (атм)'
], axis=1)

y = dataml_group[[
  'объем нефти (м3/сутки)',
  'объем жидкости (м3/сутки)',
  'объем газа (м3/сутки)',
  'объем воды (м3/сутки)',
  'обводненность (%)',
  'рабочее время',
  'динамический уровень (м)',
  'пластовое давление (атм)'
]]

# Разделение на тренировочную (80%), тестовую (10%) и валидационную (10%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1024, shuffle=True)
X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=32, shuffle=True)
X_values = [
  'погодное условие',
  'направление ветра',
  'скорость ветра (м/с)',
  'давление (мм рт. ст.)',
  'влажность (%)',
  'видимость (мм)',
  'луна',
  'осадки (мм)',
  'температура днем',
  'температура ночью'
]
y_values = [
  'пластовое давление (атм)'
]

X_train_original_1 = X_train
y_train_original_1 = y_train
X_test_original_1 = X_test
y_test_original_1 = y_test

# Случайный лес
RandomForest = RandomForestRegressor(n_estimators=100)
RandomForest.fit(X_train[X_values], y_train[y_values])
print(f"Тренировочный набор - {RandomForest.score(X_train[X_values], y_train[y_values])*100}")
print(f"Тестовый набор - {RandomForest.score(X_test[X_values], y_test[y_values])*100}")

# Как видно, качество повысилось, благодаря группировки данных и изучению предметной области

# Bagging
Bagging = BaggingRegressor()
Bagging.fit(X_train[X_values], y_train[y_values])
print(f"Тренировочный набор - {Bagging.score(X_train[X_values], y_train[y_values])*100}")
print(f"Тестовый набор - {Bagging.score(X_test[X_values], y_test[y_values])*100}")

# AdaBoost
AdaBoost = AdaBoostRegressor(n_estimators=100)
AdaBoost.fit(X_train[X_values], y_train[y_values])
print(f"Тренировочный набор - {AdaBoost.score(X_train[X_values], y_train[y_values])*100}")
print(f"Тестовый набор - {AdaBoost.score(X_test[X_values], y_test[y_values])*100}")

# CatBoost
CatBoost = CatBoostRegressor(n_estimators=100)
CatBoost.fit(X_train[X_values], y_train[y_values])
print(f"Тренировочный набор - {CatBoost.score(X_train[X_values], y_train[y_values])*100}")
print(f"Тестовый набор - {CatBoost.score(X_test[X_values], y_test[y_values])*100}")

# XGBoost
XGBoost = XGBRegressor(n_estimators=100)
XGBoost.fit(X_train[X_values], y_train[y_values])
print(f"Тренировочный набор - {XGBoost.score(X_train[X_values], y_train[y_values])*100}")
print(f"Тестовый набор - {XGBoost.score(X_test[X_values], y_test[y_values])*100}")

"""# На данный момент, лучшие результаты показывают алгоритмы: RandomForest и AdaBoost. Использую Feature engineering."""

plt.rcParams["figure.figsize"] = (12, 6)
plt.style.use('fivethirtyeight')
X.plot(subplots=True, sharex=True, figsize=(20, 15))
plt.show()

X_drop = X_original_1
X_drop['id'] = X_drop.index
X_drop = X_drop.drop(['год', 'месяц', 'день'], axis=1)
X_drop.describe().T

# Минимальный параметр
settings_min = settings.MinimalFCParameters()
settings_min

extracted_features = extract_features(X_drop, column_id="id", impute_function=impute, default_fc_parameters=settings_min)

extracted_features

extracted_features.describe().T

# доля пропусков
# проверяю параметры на 0. Если вся строка 0 - отфильтровываем
extracted_features.isnull().sum().sum()

# смотрю пропуски
extracted_features.isnull().mean()

# БЕЗ ГРУППИРОВКИ
# разделим данные  на тестовую (20%) и тренировочную выборки (80%)
X_train_ext, X_test_ext, y_train_ext, y_test_ext = train_test_split(extracted_features, y_original_1, test_size=0.2, random_state=1024, shuffle=True)
X_test_ext, X_val_ext, y_test_ext, y_val_ext = train_test_split(extracted_features, y_original_1, test_size=0.5, random_state=32, shuffle=True)

# X_values = [
#   'погодное условие',
#   'направление ветра',
#   'скорость ветра (м/с)',
#   'давление (мм рт. ст.)',
#   'влажность (%)',
#   'видимость (мм)',
#   'луна',
#   'осадки (мм)',
#   'температура днем',
#   'температура ночью'
# ]
y_values = [
  'пластовое давление (атм)'
]

# Случайный лес
forest = RandomForestRegressor(n_estimators=100)
forest.fit(X_train_ext, y_train_ext[y_values])
print(f"Тренировочный набор - {forest.score(X_train_ext, y_train_ext[y_values])*100}")
print(f"Тестовый набор - {forest.score(X_test_ext, y_test_ext[y_values])*100}")

# С ГРУППИРОВКОЙ
X_drop = X
X_drop['id'] = X_drop.index
X_drop = X_drop.drop(['день'], axis=1)

extracted_features = extract_features(X_drop, column_id="id", impute_function=impute, default_fc_parameters=settings_min)

# разделим данные  на тестовую (20%) и тренировочную выборки (80%)
X_train_ext, X_test_ext, y_train_ext, y_test_ext = train_test_split(extracted_features, y, test_size=0.2, random_state=1024, shuffle=True)
X_test_ext, X_val_ext, y_test_ext, y_val_ext = train_test_split(extracted_features, y, test_size=0.5, random_state=32, shuffle=True)

# X_values = [
#   'погодное условие',
#   'направление ветра',
#   'скорость ветра (м/с)',
#   'давление (мм рт. ст.)',
#   'влажность (%)',
#   'видимость (мм)',
#   'луна',
#   'осадки (мм)',
#   'температура днем',
#   'температура ночью'
# ]
y_values = [
  'пластовое давление (атм)'
]

X_train_original_2 = X_train_ext
y_train_original_2 = y_train_ext
X_test_original_2 = X_test_ext
y_test_original_2 = y_test_ext

# Случайный лес
forest = RandomForestRegressor(n_estimators=100)
forest.fit(X_train_ext, y_train_ext[y_values])
print(f"Тренировочный набор - {forest.score(X_train_ext, y_train_ext[y_values])*100}")
print(f"Тестовый набор - {forest.score(X_test_ext, y_test_ext[y_values])*100}")

"""# Получил показатели благодаря изучению предметной области, группировки данных по году и месяцу, а также применению Feature engineering:
# Тренировочный набор - 91.18210195604976
# Тестовый набор - 86.3432308966019
"""

# Список признаков с весами
feature_names_str = [f"col{col}" for col in extracted_features.columns.tolist()]
cols = [col.replace("col", "") for col in feature_names_str[:12]]
cols

# Вес каждого фактора в итоговой моделей
forest.feature_importances_

plot = pd.Series(data=forest.feature_importances_).plot(kind='bar')
plot.tick_params(axis='x', rotation=90)

# Сохраню модели
joblib.dump(RandomForest, '/content/model.pkl') # без Feature engineering
joblib.dump(forest, '/content/model_fe.pkl') # с Feature engineering

# Загружу модели
RandomForest = joblib.load('/content/model.pkl') # без Feature engineering
forest = joblib.load('/content/model_fe.pkl') # с Feature engineering

# Использую загруженные модели для предсказания
# без Feature engineering
# Тренировочный набор - 91.07214646869724
# Тестовый набор - 53.455281999856
X_test_pred = X_test_original_1.drop(['день'], axis=1)
y_pred = RandomForest.predict(X_test_pred)
y_pred

# Использую загруженные модели для предсказания
# с Feature engineering
# Тренировочный набор - 91.25531053440886
# Тестовый набор - 88.85372762794663
y_pred = forest.predict(X_test_original_2)
y_pred

# Пример ввода собственных данных
# без Feature engineering
y_pred = RandomForest.predict([[
  1, # погодное условие
  1, # направление ветра
  1, # скорость ветра (м/с)
  1, # давление (мм рт. ст.)
  1, # влажность (%)
  1, # видимость (мм)
  1, # луна
  1, # осадки (мм)
  1, # температура днем
  1, # температура ночью
]])
y_pred

# Пример ввода собственных данных
# с Feature engineering
y_pred = forest.predict([[
  1, # погодное условие__sum_values
  1, # погодное условие__median
  1, # погодное условие__mean
  1, # погодное условие__length
  1, # погодное условие__standard_deviation
  1, # погодное условие__variance
  1, # погодное условие__root_mean_square
  1, # погодное условие__maximum
  1, # погодное условие__absolute_maximum
  1, # погодное условие__minimum
  1, # направление ветра__sum_values
  1, # направление ветра__median
  1, # направление ветра__mean
  1, # направление ветра__length
  1, # направление ветра__standard_deviation
  1, # направление ветра__variance
  1, # направление ветра__root_mean_square
  1, # направление ветра__maximum
  1, # направление ветра__absolute_maximum
  1, # направление ветра__minimu
  1, # скорость ветра (м/с)__sum_values
  1, # скорость ветра (м/с)__median
  1, # скорость ветра (м/с)__mean
  1, # скорость ветра (м/с)__length
  1, # скорость ветра (м/с)__standard_deviation
  1, # скорость ветра (м/с)__variance
  1, # скорость ветра (м/с)__root_mean_square
  1, # скорость ветра (м/с)__maximum
  1, # скорость ветра (м/с)__absolute_maximum
  1, # скорость ветра (м/с)__minimum
  1, # давление (мм рт. ст.)__sum_values
  1, # давление (мм рт. ст.)__median
  1, # давление (мм рт. ст.)__mean
  1, # давление (мм рт. ст.)__length
  1, # давление (мм рт. ст.)__standard_deviation
  1, # давление (мм рт. ст.)__variance
  1, # давление (мм рт. ст.)__root_mean_square
  1, # давление (мм рт. ст.)__maximum
  1, # давление (мм рт. ст.)__absolute_maximum
  1, # давление (мм рт. ст.)__minimum
  1, # влажность (%)__sum_values
  1, # влажность (%)__median
  1, # влажность (%)__mean
  1, # влажность (%)__length
  1, # влажность (%)__standard_deviation
  1, # влажность (%)__variance
  1, # влажность (%)__root_mean_square
  1, # влажность (%)__maximum
  1, # влажность (%)__absolute_maximum
  1, # влажность (%)__minimum
  1, # видимость (мм)__sum_values
  1, # видимость (мм)__median
  1, # видимость (мм)__mean
  1, # видимость (мм)__length
  1, # видимость (мм)__standard_deviation
  1, # видимость (мм)__variance
  1, # видимость (мм)__root_mean_square
  1, # видимость (мм)__maximum
  1, # видимость (мм)__absolute_maximum
  1, # видимость (мм)__minimum
  1, # луна__sum_values
  1, # луна__median
  1, # луна__mean
  1, # луна__length
  1, # луна__standard_deviation
  1, # луна__variance
  1, # луна__root_mean_square
  1, # луна__maximum
  1, # луна__absolute_maximum
  1, # луна__minimum
  1, # осадки (мм)__sum_values
  1, # осадки (мм)__median
  1, # осадки (мм)__mean
  1, # осадки (мм)__length
  1, # осадки (мм)__standard_deviation
  1, # осадки (мм)__variance
  1, # осадки (мм)__root_mean_square
  1, # осадки (мм)__maximum
  1, # осадки (мм)__absolute_maximum
  1, # осадки (мм)__minimum
  1, # температура днем__sum_values
  1, # температура днем__median
  1, # температура днем__mean
  1, # температура днем__length
  1, # температура днем__standard_deviation
  1, # температура днем__variance
  1, # температура днем__root_mean_square
  1, # температура днем__maximum
  1, # температура днем__absolute_maximum
  1, # температура днем__minimum
  1, # температура ночью__sum_values
  1, # температура ночью__median
  1, # температура ночью__mean
  1, # температура ночью__length
  1, # температура ночью__standard_deviation
  1, # температура ночью__variance
  1, # температура ночью__root_mean_square
  1, # температура ночью__maximum
  1, # температура ночью__absolute_maximum
  1, # температура ночью__minimum
]])
y_pred

"""# Итог:
### Задав новые данные погодных условий:
###   'погодное условие',
###   'направление ветра',
###   'скорость ветра (м/с)',
###   'давление (мм рт. ст.)',
###   'влажность (%)',
###   'видимость (мм)',
###   'луна',
###   'осадки (мм)',
###   'температура днем',
###   'температура ночью'
### .
### Получу результат:
###   'пластовое давление (атм)'
### .
### Это давление влияет на увеличение давления нефти. Что может привести к поломке оборудования нефтяной скважины №807.
"""