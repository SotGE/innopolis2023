# -*- coding: utf-8 -*-
"""lesson13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O_158GPJzEk6eE0KF0KtQa1Kjb_yKt2s

# Метрики TF-IDF для любых 10 песен
"""

!pip install pymorphy3
!pip install transformers

import os # работа с папкой и файлами
import re # регулярные выражения, доп вариант к очистке
import string # работа со строкой
import math
import operator
import random
import time

import numpy as np
import pandas as pd # работа с таблицей dataframe

import nltk # работа с пакетами языков
from nltk import word_tokenize, ngrams # токенизация и деление на n граммы
from nltk.corpus import stopwords # стопслова, extend

from wordcloud import WordCloud # визуальное отображение
import pymorphy3 # работа с русским языком, pymorphy3

import matplotlib.pyplot as plt # визуальное отображение
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

# скачаем словарь стоп-слов
nltk.download("stopwords")

# скачиваем модель, которая будет делить на предложения
nltk.download('punkt')

from sklearn.model_selection import train_test_split # Деление выборки на тестовые и тренировочные данные

# python -m pip install transformers
from transformers import BertTokenizer # токенизатор BERT
from transformers import BertModel
from transformers import AdamW, get_linear_schedule_with_warmup

# python -m pip install tensorflow
# python -m pip install torch torchvision torchaudio
import tensorflow
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score, f1_score # Критерий качества, точности

# from google.colab import drive
# drive.mount('/content/drive')

!wget 'https://raw.githubusercontent.com/SotGE/innopolis2023/main/lesson13/data.zip' -O 'data.zip'
!unzip -o '/content/data.zip' -d '/content/data'

# Список файлов в папке
FOLDER_NAME = 'data'

direct_list = os.listdir(FOLDER_NAME)
direct_list

# Имена песен из названия файлов
names = [song.replace(".txt", "") for song in direct_list]
names

# Функция чтения файла
def read_song(filename, folder=FOLDER_NAME):
  text = ''
  with open(f'{folder}/{filename}.txt', 'r', encoding='utf-8') as f:
    text = f.read()
    text = text.replace('\n', ' ')
  return text

# Чтение данных из папки
songs_list = []

for song in names:
  songs_list.append(read_song(song))

songs_list

# Стоп-слова русского языка
stopwords_list = stopwords.words("russian")

# Собственные стоп-слова
extra_stop = ['твоей', 'свои', 'тобой', 'мой', 'твой', 'мой', 'это']
stopwords_list.extend(extra_stop)

len(stopwords_list)

# Очищение строк
def clean_string(text):
  string.punctuation += '—'
  text = re.split(' |:|\.|\(|\)|,|"|;|/|\n|\t|-|\?|\[|\]|!|…', text)
  text = ' '.join([word for word in text if word not in string.punctuation])
  text = text.lower()
  text = ' '.join([word for word in text.split() if word not in stopwords_list])
  return text

songs_list_сlean = [clean_string(song) for song in songs_list]
songs_list_сlean

# Приводим слова к начальной форме
# py -m pip install pymorphy3-dicts-ru
morph = pymorphy3.MorphAnalyzer(lang='ru')

def normilize_word(text):
  words = text.split() # по пробелу разделить
  result_list = []
  for word in words:
    # print(morph.parse(word))
    normal_form = morph.parse(word)[0].normal_form
    result_list.append(normal_form)
    # print(normal_form)
  return " ".join(result_list)

songs_list_norm = [normilize_word(song) for song in songs_list_сlean]
songs_list_norm

# Имена и песни заносим во фрейм
df = pd.DataFrame({'name': names, 'song_text': songs_list_norm})
df.head()

# График в виде "облако слов"
plt.rcParams["figure.figsize"] = [40, 80]
fig = plt.figure()
i = 1
col = 3
row = math.ceil(len(df)/col)*3
for name, text in zip(df.name, df.song_text):
  tokens = word_tokenize(text)
  text_raw = " ".join(tokens)
  wordcloud = WordCloud(
    colormap='Dark2',
    background_color='white',
    contour_width=10,
    random_state=1,
    margin=10,
    collocations=False
  ).generate(text_raw)
  plt.subplot(row, col, i, label=name, frameon=True)
  plt.imshow(wordcloud, interpolation="bilinear")
  plt.axis("off")
  plt.title(name, fontdict={'fontsize':24,'color':'white', 'backgroundcolor': 'grey'}, y=1)
  i += 1

# Облако слов
wordcloud.layout_

# Создание объекта TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Применение TF-IDF к текстовым данным
tfidf_matrix = tfidf_vectorizer.fit_transform(songs_list_norm)

# Получение списка ключевых слов и их значения TF-IDF для первого документа
feature_names = tfidf_vectorizer.get_feature_names_out()
tfidf_scores = tfidf_matrix.toarray()[0]

# Сортировка слов по значениям TF-IDF
sorted_keywords = [word for _, word in sorted(zip(tfidf_scores, feature_names), reverse=True)]

print("Ключевые слова:", sorted_keywords)

# Список ключевых слов
feature_names

# Значение ключевых слов
df = pd.DataFrame(tfidf_scores, columns=['TF-IDF'])
df.head(20)

# Ограничение количества признаков
count_vectorizer = CountVectorizer()
bow = count_vectorizer.fit_transform(songs_list_norm)
print(bow.shape)
vocab_sorted = sorted(count_vectorizer.vocabulary_.items(), key=operator.itemgetter(0))
vocab_sorted

# Биграммы, триграммы, n-граммы
count_vectorizer = CountVectorizer(ngram_range=(1,2),  min_df=2)
bow = count_vectorizer.fit_transform(songs_list_norm)
count_vectorizer.vocabulary_

"""# Использование предварительно обученной модели BERT для классификации тональности отзывов на фильмы"""

# Чтение данных
url = f"https://raw.githubusercontent.com/SotGE/innopolis2023/main/lesson13/IMDB Dataset.csv"
url = url.replace(" ", "%20")
df = pd.read_csv(url)
# df = pd.read_csv(f"IMDB Dataset.csv")
df

# Бинарная категоризация столбца "sentiment"
df.sentiment = pd.Categorical(df.sentiment)
df.sentiment = df.sentiment.cat.codes
df

# Разделим данные на тренировочные и тестовые
x = df['review']
y = df['sentiment']
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state=15)

# Определим токенизатор BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
tokenizer

# Возьмем длину для Берта - 256
MAX_LEN = 256

def preprocessing_for_bert(data):
  """Препроцессинг на модели  BERT.
  @param    data (np.array): Текст для препроц.
  @return   input_ids (torch.Tensor): Либо id token, либо объект тензор.
  @return   attention_masks (torch.Tensor): Определяем набор токенов (объекты тензоры) для работы мехонизма внимания.
  """
  input_ids = []
  attention_masks = []
  for sent in data:
    # `encode_plus` will:
    #    (1) Tokenize the sentence
    #    (2) Add the `[CLS]` and `[SEP]` token to the start and end
    #    (3) Truncate/Pad sentence to max length
    #    (4) Map tokens to their IDs
    #    (5) Create attention mask
    #    (6) Return a dictionary of outputs
    encoded_sent = tokenizer.encode_plus(
      sent,
      add_special_tokens=True,        # Add `[CLS]` and `[SEP]`
      max_length=MAX_LEN,             # Max length to truncate/pad
      pad_to_max_length=True,         # Pad sentence to max length
      return_attention_mask=True      # Return attention mask
    )

    # Добавление обработанных (закодированных данных) в списки id, masks
    input_ids.append(encoded_sent.get('input_ids'))
    attention_masks.append(encoded_sent.get('attention_mask'))
  # Преобразуем в тензоры
  input_ids = torch.tensor(input_ids)
  attention_masks = torch.tensor(attention_masks)

  return input_ids, attention_masks

# Используем токенизатор BERT для подготовки тренировочных и тестовых данных для классификатора BERT
# Выполняем на T4 GPU
train_inputs, train_masks = preprocessing_for_bert(x_train)
test_inputs, test_masks = preprocessing_for_bert(x_test)

train_masks

train_inputs

# Подготовим другие данные для использования в классификаторе BERT
# преобразуем объект Series в массив
if isinstance(y_train, pd.Series):
  y_train=y_train.to_numpy() # преобразуем объект Series в массив

if isinstance(y_test, pd.Series):
  y_test=y_test.to_numpy() # преобразуем объект Series в массив

train_labels = torch.LongTensor(y_train) # делаем из numpy array LongTensor
test_labels = torch.LongTensor(y_test) # делаем из numpy array LongTensor

batch_size = 32

# TensorDataset: входные данные (отзывы), маски для отзывов, с чем сравнивать
train_data = TensorDataset(train_inputs, train_masks, train_labels)
# RandomSampler: входные данные (отзывы)
train_sampler = RandomSampler(train_data)
# RandomSampler: входные данные (отзывы), batch_size, RandomSampler
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

test_data = TensorDataset(test_inputs, test_masks, test_labels)
test_sampler = SequentialSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)

# Определим классификатор BERT
class BertClassifier(nn.Module):
  def __init__(self, freeze_bert=False):
    super(BertClassifier, self).__init__()

    D_in, H, D_out = 768, 50, 2

    self.bert = BertModel.from_pretrained('bert-base-uncased')

    self.classifier = nn.Sequential(
      nn.Linear(D_in, H),
      nn.ReLU(),
      #nn.Dropout(0.3),
      nn.Linear(H, D_out)
    )

    if freeze_bert:
      for param in self.bert.parameters():
        param.requires_grad = False

  def forward(self, input_ids, attention_mask):
    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
    # Смотрим CLS
    last_hidden_state_cls = outputs[0][:, 0, :]

    logits = self.classifier(last_hidden_state_cls)
    return logits

# Инициализируем модель
EPOCH_NUM = 4
device = torch.device("cuda")
# device = torch.device("cpu")

def initialize_model(epochs=EPOCH_NUM):
  # экземпляр BertClassifier
  bert_classifier = BertClassifier(freeze_bert=False)

  # Задаем устройство (GPU T4)
  bert_classifier.to(device)

  # Create the optimizer
  optimizer = AdamW(bert_classifier.parameters(),
    lr=5e-5,    # learning rate
    eps=1e-8    # epsilon - точность функции, до какого значения считать
  )

  # Расчет циклов для работы
  total_steps = len(train_dataloader) * epochs

  # Загрузчик, расписание (последовательность) и число циклов для обучения
  scheduler = get_linear_schedule_with_warmup(optimizer,
    num_warmup_steps=0, # Default value
    num_training_steps=total_steps)
  return bert_classifier, optimizer, scheduler

# Определим функцию вычисления показателей качества модели (функцию потерь и точность)
def evaluate(model, test_dataloader):

  model.eval()

  test_accuracy = []
  test_loss = []

  for batch in test_dataloader:
    b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

    with torch.no_grad():
      logits = model(b_input_ids, b_attn_mask)

    loss = loss_fn(logits, b_labels)
    test_loss.append(loss.item())

    preds = torch.argmax(logits, dim=1).flatten()

    accuracy = (preds == b_labels).cpu().numpy().mean() * 100
    test_accuracy.append(accuracy)

  test_loss = np.mean(test_loss)
  test_accuracy = np.mean(test_accuracy)

  return test_loss, test_accuracy

# Определим функцию обучения модели
# Функция потерь (бинарная классификация)
loss_fn = nn.CrossEntropyLoss()

def set_seed(seed_value=1):
  '''
  Воспроизводимость модели, по умолчанию seed_value=42
  '''
  random.seed(seed_value)
  np.random.seed(seed_value)
  torch.manual_seed(seed_value)
  torch.cuda.manual_seed_all(seed_value)

def train(model, train_dataloader, test_dataloader=None, epochs=EPOCH_NUM, evaluation=False):
  """
  Тренировка BertClassifier model
  """
  print("Начало тренировки...\n")
  for epoch_i in range(epochs):
    # =======================================
    #               Training
    # =======================================
    # Print the header of the result table
    print(f"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Test Loss':^10} | {'Test Acc':^9} | {'Elapsed':^9}")
    print("-"*70)

    # Измерение времени (elapsed time of each epoch)
    t0_epoch, t0_batch = time.time(), time.time()

    # Обнулить переменные
    total_loss, batch_loss, batch_counts = 0, 0, 0

    # Перевод в режим тренировки
    model.train()

    # Для каждого batch
    for step, batch in enumerate(train_dataloader):
      batch_counts +=1
      # Загружаем каждую batch в GPU
      b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

      # Использование нулевого градиента
      model.zero_grad()

      # Добавляем в модель id и маски
      logits = model(b_input_ids, b_attn_mask)

      # Считаем потери (Кросс Энтропия)
      loss = loss_fn(logits, b_labels)
      batch_loss += loss.item()
      total_loss += loss.item()

      # Расчет градиентов функцией backward
      loss.backward()

      # Решение проблемы "взрывных градиентов"
      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

      # Обновляем шаг для оптимизатора и расписания
      optimizer.step()
      scheduler.step()

      # Печатаем значения для каждых 20 batch
      if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):
        # Время расчета 20 batches
        time_elapsed = time.time() - t0_batch

        # Результаты
        print(f"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}")

        # Обнулим переменные
        batch_loss, batch_counts = 0, 0
        t0_batch = time.time()

    # Средняя функция потерь
    avg_train_loss = total_loss / len(train_dataloader)

    print("-"*70)
    # =======================================
    #               Оценка
    # =======================================
    if evaluation == True:
      # Считается точность
      test_loss, test_accuracy = evaluate(model, test_dataloader)

      # Общее время, затраченное на измерение
      time_elapsed = time.time() - t0_epoch

      print(f"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {test_loss:^10.6f} | {test_accuracy:^9.2f} | {time_elapsed:^9.2f}")
      print("-"*70)
    print("\n")

  print("Тренировка завершена!")

# Выполним обучение модели
set_seed(42)    # Set seed for reproducibility
bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCH_NUM)
train(bert_classifier, train_dataloader, test_dataloader, epochs=EPOCH_NUM, evaluation=True)

# Выгружаем модель
torch.save(bert_classifier, f"/content/model.pth")
torch.save(bert_classifier.state_dict(), f"/content/model_state_dict.pth")

# Загружаем натренированную модель
bert_classifier = torch.load(f"/content/model.pth")
bert_classifier.eval()

# bert_classifier = TheModelClass(*args, **kwargs)
# bert_classifier.load_state_dict(torch.load(f"/content/model.pth"))
# bert_classifier.eval()

# Определим функцию предсказания положительного/отрицательного отзыва с использованием обученной модели
def bert_predict(model, test_dataloader):
    model.eval()

    all_logits = []

    for batch in test_dataloader:
        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]

        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)
        all_logits.append(logits)

    # Сбор из каждого batch результатов в один объект
    all_logits = torch.cat(all_logits, dim=0)

    # Для вероятности принадлежности к классу
    probs = F.softmax(all_logits, dim=1).cpu().numpy()

    return probs

# Commented out IPython magic to ensure Python compatibility.
# Построим график ROC-кривой для наглядной оценки качества предсказания
# %matplotlib inline

def evaluate_roc(probs, y_true):
    """
    - Print AUC and accuracy on the test set
    - Plot ROC
    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)
    @params    y_true (np.array): an array of the true values with shape (len(y_true),)
    """
    preds = probs[:, 1]
    fpr, tpr, threshold = roc_curve(y_true, preds)
    roc_auc = auc(fpr, tpr)
    print(f'AUC: {roc_auc:.4f}')

    # Get accuracy over the test set
    y_pred = np.where(preds >= 0.5, 1, 0)
    accuracy = accuracy_score(y_true, y_pred)
    print(f'Accuracy: {accuracy*100:.2f}%')

    # Plot ROC AUC
    plt.figure(figsize=(10, 8))
    plt.title('Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.show()

# Compute predicted probabilities on the test set
probs = bert_predict(bert_classifier, test_dataloader)

# Evaluate the Bert classifier
evaluate_roc(probs, y_test)